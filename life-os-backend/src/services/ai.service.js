const { GoogleGenerativeAI } = require("@google/generative-ai");
const { GoogleAIFileManager } = require("@google/generative-ai/server");
const axios = require('axios');
require('dotenv').config();

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const fileManager = new GoogleAIFileManager(process.env.GEMINI_API_KEY);

class AIService {
  constructor() {
    this.generationConfig = {
      temperature: 1,
      topP: 0.95,
      topK: 64,
      maxOutputTokens: 8192,
      responseMimeType: "application/json",
    };
    // Default model
    this.defaultModelId = "gemini-2.0-flash";
    this.fallbackModels = ["gemini-2.0-flash", "gemini-2.5-flash", "gemini-pro-latest"];
    this.modelHealth = {}; // Stores { modelId: { status: 'ok'|'error', reason: string, lastTried: Date } }
  }

  async listModels() {
    try {
      const apiKey = process.env.GEMINI_API_KEY;
      const response = await axios.get(`https://generativelanguage.googleapis.com/v1beta/models?key=${apiKey}`);

      // List of keywords to exclude (specialized models that don't support text-only output)
      const excludedKeywords = [
        'tts', 'audio', 'image', 'embedding', 'aqa',
        'imagen', 'veo', 'computer-use', 'deep-research',
        'bidi', 'nano-banana'
      ];

      // Map custom functional descriptions for better UX
      const modelDescriptions = {
        'gemini-2.5-flash': 'Model Flash má»›i nháº¥t, cÃ¢n báº±ng tuyá»‡t vá»i giá»¯a tá»‘c Ä‘á»™ vÃ  Ä‘á»™ thÃ´ng minh.',
        'gemini-2.5-pro': 'Máº¡nh máº½ nháº¥t cho cÃ¡c tÃ¡c vá»¥ suy luáº­n phá»©c táº¡p vÃ  tÃ³m táº¯t bÃ i giáº£ng dÃ i.',
        'gemini-2.0-flash': 'Cá»±c nhanh vÃ  Ä‘a nÄƒng, phÃ¹ há»£p cho háº§u háº¿t cÃ¡c tÃ¡c vá»¥ hÃ ng ngÃ y.',
        'gemini-2.0-flash-lite': 'PhiÃªn báº£n siÃªu nháº¹, tá»‘c Ä‘á»™ cao vÃ  cá»±c ká»³ á»•n Ä‘á»‹nh.',
        'gemini-1.5-flash': 'Model Flash Ä‘á»i cÅ©, á»•n Ä‘á»‹nh cho cÃ¡c tÃ¡c vá»¥ cÆ¡ báº£n.',
        'gemini-1.5-pro': 'Há»— trá»£ ngá»¯ cáº£nh cá»±c lá»›n (vÅ© trá»¥ 2 triá»‡u tokens).',
        'gemini-pro-latest': 'LuÃ´n trá» Ä‘áº¿n phiÃªn báº£n Pro máº¡nh nháº¥t hiá»‡n táº¡i.',
        'gemini-flash-latest': 'LuÃ´n trá» Ä‘áº¿n phiÃªn báº£n Flash nhanh nháº¥t hiá»‡n táº¡i.'
      };

      // Filter models that support generateContent AND are not specialized
      return response.data.models
        .filter(m => {
          const name = m.name.toLowerCase();
          const supportsGenerate = m.supportedGenerationMethods.includes('generateContent');
          const isSpecialized = excludedKeywords.some(keyword => name.includes(keyword));
          return supportsGenerate && !isSpecialized;
        })
        .map(m => {
          const id = m.name.split('/').pop();
          return {
            id: id,
            displayName: m.displayName,
            description: modelDescriptions[id] || m.description,
            inputTokenLimit: m.inputTokenLimit,
            outputTokenLimit: m.outputTokenLimit,
            health: this.modelHealth[id] || { status: 'ok', reason: null }
          };
        });
    } catch (error) {
      console.error("âŒ Error listing models:", error.message);
      return [
        { id: "gemini-2.0-flash", displayName: "Gemini 2.0 Flash (Fallback)", description: "Fast and versatile" },
        { id: "gemini-2.5-flash", displayName: "Gemini 2.5 Flash (Fallback)", description: "Mid-size multimodal model" }
      ];
    }
  }

  async analyzeText(text, modelId = null) {
    // Basic validation for modelId
    const excludedKeywords = ['tts', 'audio', 'image', 'embedding', 'bidi'];
    if (modelId && excludedKeywords.some(kw => modelId.toLowerCase().includes(kw))) {
      console.warn(`âš ï¸ Warning: Model ${modelId} appears to be specialized for non-text tasks. Falling back to default.`);
      modelId = null;
    }

    const modelsToTry = modelId
      ? [modelId, ...this.fallbackModels.filter(m => m !== modelId)]
      : this.fallbackModels;

    let lastError = null;

    for (const selectedModelId of modelsToTry) {
      try {
        let modelName = selectedModelId;
        if (modelName.startsWith('models/')) {
          modelName = modelName.replace('models/', '');
        }

        console.log(`ðŸ¤– Attempting analysis with model: ${modelName}`);

        const model = genAI.getGenerativeModel({
          model: modelName,
          generationConfig: this.generationConfig
        });

        const prompt = `
          Báº¡n lÃ  chuyÃªn gia ngÃ´n ngá»¯. HÃ£y phÃ¢n tÃ­ch vÄƒn báº£n: "${text}"
          
          LÆ¯U Ã QUAN TRá»ŒNG: Náº¿u vÄƒn báº£n Ä‘áº§u vÃ o cÃ³ chá»©a cÃ¡c má»‘c thá»i gian dáº¡ng [15s], [120s] v.v... á»Ÿ Ä‘áº§u má»—i cÃ¢u (thÆ°á»ng lÃ  transcript tá»« video), báº¡n PHáº¢I trÃ­ch xuáº¥t vÃ  tráº£ vá» con sá»‘ thá»i gian Ä‘Ã³ vÃ o trÆ°á»ng "timestamp" cho má»—i tá»« vá»±ng mÃ  báº¡n tÃ¬m Ä‘Æ°á»£c trong cÃ¢u tÆ°Æ¡ng á»©ng. Náº¿u khÃ´ng cÃ³ má»‘c thá»i gian, hÃ£y Ä‘á»ƒ null.
          
          Äá»I Vá»šI DICTATION: Báº¡n pháº£i trÃ­ch xuáº¥t toÃ n bá»™ cÃ¡c cÃ¢u thoáº¡i hoáº·c Ã­t nháº¥t lÃ  cÃ¡c cÃ¢u quan trá»ng thÃ nh má»™t danh sÃ¡ch "sentences". Má»—i cÃ¢u cáº§n cÃ³ pháº§n tiáº¿ng Anh nguyÃªn báº£n, pháº§n dá»‹ch nghÄ©a tiáº¿ng Viá»‡t vÃ  má»‘c thá»i gian báº¯t Ä‘áº§u.

          YÃªu cáº§u output (JSON schema):
          {
            "summary": "string (tÃ³m táº¯t ngáº¯n gá»n)",
            "keywords": ["string", "string"],
            "difficulty": "string (Dá»…/Trung bÃ¬nh/KhÃ³)",
            "vocabularyList": [
              { 
                "word": "string (tá»« gá»‘c)", 
                "ipa": "string (phiÃªn Ã¢m quá»‘c táº¿)",
                "definition": "string (nghÄ©a tiáº¿ng Viá»‡t)",
                "example": "string (cÃ¢u vÃ­ dá»¥ tiáº¿ng Anh)",
                "synonyms": ["string"],
                "timestamp": "number hoáº·c null (sá»‘ giÃ¢y xuáº¥t hiá»‡n trong video gá»‘c náº¿u cÃ³)"
              }
            ],
            "sentences": [
              {
                "text": "string (cÃ¢u thoáº¡i tiáº¿ng Anh nguyÃªn báº£n)",
                "translation": "string (nghÄ©a tiáº¿ng Viá»‡t cá»§a cÃ¢u)",
                "timestamp": number (sá»‘ giÃ¢y báº¯t Ä‘áº§u cÃ¢u thoáº¡i)
              }
            ]
          }
        `;

        const result = await model.generateContent(prompt);
        const response = await result.response;
        const textResponse = response.text();
        const jsonData = JSON.parse(textResponse);

        // Record success
        this.modelHealth[modelName] = { status: 'ok', reason: null, lastTried: new Date() };

        return jsonData;

      } catch (error) {
        lastError = error;
        console.error(`âš ï¸ AI Service Error (Model: ${selectedModelId}):`, error.message);

        let reason = "Error";
        if (error.message.includes('429') || error.message.includes('quota')) {
          reason = "Quota Exceeded (429)";
          console.warn(`ðŸ”„ Quota exceeded for ${selectedModelId}. Trying next fallback model...`);
        } else if (error.message.includes('404')) {
          reason = "Model Not Found (404)";
        } else {
          reason = error.message.substring(0, 50);
        }

        // Record failure
        this.modelHealth[selectedModelId.replace('models/', '')] = {
          status: 'error',
          reason: reason,
          lastTried: new Date()
        };

        if (reason.includes('Quota')) {
          continue; // Chuyá»ƒn sang model tiáº¿p theo
        }

        // Náº¿u khÃ´ng pháº£i lá»—i quota thÃ¬ break luÃ´n (hoáº·c cÃ³ thá»ƒ thá»­ tiáº¿p tÃ¹y logic)
        break;
      }
    }

    // Náº¿u táº¥t cáº£ cÃ¡c model Ä‘á»u tháº¥t báº¡i
    return {
      summary: "KhÃ´ng thá»ƒ phÃ¢n tÃ­ch lÃºc nÃ y (Lá»—i AI/Quota)",
      keywords: [],
      difficulty: "N/A",
      vocabularyList: []
    };
  }

  async analyzeMedia(filePath, modelId = null) {
    // Media only works well directly with 1.5 Pro, 1.5 Flash, or 2.0 Flash
    const modelsToTry = modelId
      ? [modelId, ...this.fallbackModels.filter(m => m !== modelId)]
      : this.fallbackModels;

    let uploadResult = null;
    try {
      // 1. Upload file using GoogleAIFileManager
      console.log(`ðŸ“¡ Äang táº£i file lÃªn Gemini API: ${filePath}`);
      const mime = require('mime-types').lookup(filePath) || 'audio/mp3';

      uploadResult = await fileManager.uploadFile(filePath, {
        mimeType: mime,
        displayName: filePath.split(/[\\/]/).pop(),
      });
      console.log(`âœ… Táº£i file thÃ nh cÃ´ng. URI: ${uploadResult.file.uri}`);

      // Chá» file chuyá»ƒn sang tráº¡ng thÃ¡i ACTIVE (Ä‘á»‘i vá»›i video/audio)
      let fileInfo = await fileManager.getFile(uploadResult.file.name);
      while (fileInfo.state === "PROCESSING") {
        console.log(`â³ Äang chá» Gemini xá»­ lÃ½ file (tráº¡ng thÃ¡i: PROCESSING)...`);
        await new Promise(resolve => setTimeout(resolve, 4000));
        fileInfo = await fileManager.getFile(uploadResult.file.name);
      }

      if (fileInfo.state === "FAILED") {
        throw new Error("QuÃ¡ trÃ¬nh xá»­ lÃ½ file cá»§a Gemini tháº¥t báº¡i.");
      }

      console.log(`âœ… File Ä‘Ã£ sáºµn sÃ ng phÃ¢n tÃ­ch (tráº¡ng thÃ¡i: ${fileInfo.state}).`);



    } catch (uploadError) {
      console.error(`âŒ Lá»—i táº£i file lÃªn Gemini API:`, uploadError);
      return {
        summary: "Lá»—i táº£i file lÃªn há»‡ thá»‘ng phÃ¢n tÃ­ch",
        keywords: [],
        difficulty: "N/A",
        vocabularyList: []
      };
    }

    for (const selectedModelId of modelsToTry) {
      try {
        let modelName = selectedModelId;
        if (modelName.startsWith('models/')) {
          modelName = modelName.replace('models/', '');
        }

        console.log(`ðŸŽ¬ Attempting MEDIA analysis with model: ${modelName}`);
        const model = genAI.getGenerativeModel({
          model: modelName,
          generationConfig: this.generationConfig
        });

        const prompt = `
          Báº¡n lÃ  chuyÃªn gia ngÃ´n ngá»¯. TÃ´i gá»­i cho báº¡n má»™t file Ã¢m thanh/video.
          HÃ£y nghe/xem vÃ  phÃ¢n tÃ­ch ná»™i dung cá»§a nÃ³.
          
          LÆ¯U Ã QUAN TRá»ŒNG Vá»€ TIMESTAMP: VÃ¬ Ä‘Ã¢y lÃ  file media, hÃ£y theo dÃµi vá»‹ trÃ­ cá»§a cÃ¡c tá»« khÃ³a vÃ  cÃ¡c cÃ¢u thoáº¡i. Báº¡n PHáº¢I trÃ­ch xuáº¥t vÃ  tráº£ vá» con sá»‘ thá»i gian xuáº¥t hiá»‡n (tÃ­nh báº±ng giÃ¢y) vÃ o trÆ°á»ng "timestamp".
          
          Äá»I Vá»šI DICTATION: Báº¡n pháº£i trÃ­ch xuáº¥t toÃ n bá»™ cÃ¡c cÃ¢u thoáº¡i hoáº·c Ã­t nháº¥t lÃ  cÃ¡c cÃ¢u quan trá»ng thÃ nh má»™t danh sÃ¡ch "sentences". Má»—i cÃ¢u cáº§n cÃ³ pháº§n tiáº¿ng Anh nguyÃªn báº£n, pháº§n dá»‹ch nghÄ©a tiáº¿ng Viá»‡t vÃ  má»‘c thá»i gian báº¯t Ä‘áº§u.

          YÃªu cáº§u output (JSON schema):
          {
            "summary": "string (tÃ³m táº¯t ná»™i dung file media)",
            "keywords": ["string", "string"],
            "difficulty": "string (Dá»…/Trung bÃ¬nh/KhÃ³)",
            "vocabularyList": [
              { 
                "word": "string (tá»« gá»‘c nghe Ä‘Æ°á»£c)", 
                "ipa": "string (phiÃªn Ã¢m quá»‘c táº¿)",
                "definition": "string (nghÄ©a tiáº¿ng Viá»‡t)",
                "example": "string (cÃ¢u vÃ­ dá»¥ tiáº¿ng Anh cÃ³ chá»©a tá»« nÃ y trong bÃ i)",
                "synonyms": ["string"],
                "timestamp": number (sá»‘ giÃ¢y xuáº¥t hiá»‡n trong file media)
              }
            ],
            "sentences": [
              {
                "text": "string (cÃ¢u thoáº¡i tiáº¿ng Anh nguyÃªn báº£n)",
                "translation": "string (nghÄ©a tiáº¿ng Viá»‡t cá»§a cÃ¢u)",
                "timestamp": number (sá»‘ giÃ¢y báº¯t Ä‘áº§u cÃ¢u thoáº¡i)
              }
            ]
          }
        `;

        const result = await model.generateContent([
          {
            fileData: {
              mimeType: uploadResult.file.mimeType,
              fileUri: uploadResult.file.uri
            }
          },
          { text: prompt }
        ]);

        const response = await result.response;
        const textResponse = response.text();
        const jsonData = JSON.parse(textResponse);

        this.modelHealth[modelName] = { status: 'ok', reason: null, lastTried: new Date() };

        // XÃ³a táº¡m (tháº­t ra API server cÅ©ng tá»± xÃ³a file sau 48h)
        try {
          await fileManager.deleteFile(uploadResult.file.name);
          console.log(`ðŸ—‘ï¸ ÄÃ£ xÃ³a file trÃªn server Gemini: ${uploadResult.file.name}`);
        } catch (e) { /* ignore */ }

        return jsonData;

      } catch (error) {
        console.error(`âš ï¸ AI Service Error (MEDIA Model: ${selectedModelId}):`, error.message);

        if (error.message.includes('Quota')) {
          continue; // Chuyá»ƒn sang model tiáº¿p theo
        }
        break;
      }
    }

    // Clean up uploaded file on failure
    if (uploadResult && uploadResult.file) {
      try { await fileManager.deleteFile(uploadResult.file.name); } catch (e) { }
    }

    return {
      summary: "KhÃ´ng thá»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u Ã¢m thanh/video lÃºc nÃ y",
      keywords: [],
      difficulty: "N/A",
      vocabularyList: []
    };
  }

  async evaluateWriting(text, targetWords = [], modelId = null) {
    const modelsToTry = modelId
      ? [modelId, ...this.fallbackModels.filter(m => m !== modelId)]
      : this.fallbackModels;

    for (const selectedModelId of modelsToTry) {
      try {
        let modelName = selectedModelId;
        if (modelName.startsWith('models/')) {
          modelName = modelName.replace('models/', '');
        }

        console.log(`ðŸ“ Attempting writing evaluation with model: ${modelName}`);

        const model = genAI.getGenerativeModel({
          model: modelName,
          generationConfig: this.generationConfig
        });

        const prompt = `
                Báº¡n lÃ  má»™t giÃ¡m kháº£o cháº¥m thi tiáº¿ng Anh chuáº©n quá»‘c táº¿ (IELTS/TOEFL) ráº¥t kháº¯t khe nhÆ°ng cÅ©ng ráº¥t táº­n tÃ¢m.
                Há»c viÃªn vá»«a gá»­i má»™t Ä‘oáº¡n vÄƒn báº£n tiáº¿ng Anh Ä‘á»ƒ báº¡n cháº¥m Ä‘iá»ƒm vÃ  nháº­n xÃ©t.
                
                Äoáº¡n vÄƒn cá»§a há»c viÃªn:
                "${text}"
                
                Danh sÃ¡ch tá»« vá»±ng má»¥c tiÃªu há»c viÃªn cáº§n sá»­ dá»¥ng (cÃ³ thá»ƒ trá»‘ng):
                [${targetWords.join(', ')}]
                
                Nhiá»‡m vá»¥ cá»§a báº¡n:
                1. ÄÃ¡nh giÃ¡ Ä‘iá»ƒm tá»•ng quan trÃªn thang Ä‘iá»ƒm 100.
                2. TÃ¬m vÃ  chá»‰ ra cÃ¡c lá»—i ngá»¯ phÃ¡p (náº¿u cÃ³).
                3. Nháº­n xÃ©t vá» cÃ¡ch há»c viÃªn sá»­ dá»¥ng cÃ¡c "tá»« vá»±ng má»¥c tiÃªu" (Ä‘Ãºng ngá»¯ cáº£nh chÆ°a, tá»± nhiÃªn chÆ°a).
                4. Cung cáº¥p 2 phiÃªn báº£n "Native Speaker Version" giá»¯ nguyÃªn Ã½ cá»§a há»c viÃªn: má»™t báº£n Formal (trang trá»ng, dÃ¹ng cho cÃ´ng viá»‡c/há»c thuáº­t) vÃ  má»™t báº£n Casual/Idiomatic (giao tiáº¿p tá»± nhiÃªn hÃ ng ngÃ y).

                YÃªu cáº§u output (JSON schema tÄ©nh, khÃ´ng tráº£ vá» markdown hay kÃ½ tá»± thá»«a nÃ o ngoÃ i JSON):
                {
                  "score": number, // Tá»« 0 Ä‘áº¿n 100
                  "grammarFeedback": [
                    {
                      "error": "string (TrÃ­ch dáº«n Ä‘oáº¡n sai)",
                      "correction": "string (CÃ¡ch sá»­a Ä‘Ãºng)",
                      "explanation": "string (Giáº£i thÃ­ch ngáº¯n gá»n báº±ng tiáº¿ng Viá»‡t)"
                    }
                  ],
                  "vocabularyUsage": "string (Nháº­n xÃ©t chung vá» cÃ¡ch dÃ¹ng tá»« vá»±ng má»¥c tiÃªu vÃ  tá»« vá»±ng nÃ³i chung báº±ng tiáº¿ng Viá»‡t. ChÃº Ã½ nháº¯c Ä‘áº¿n nhá»¯ng tá»« má»¥c tiÃªu há»c viÃªn dÃ¹ng tá»‘t hoáº·c dÃ¹ng sai)",
                  "suggestedRevisionFormal": "string (Äoáº¡n vÄƒn viáº¿t láº¡i theo phong cÃ¡ch Trang trá»ng / Formal)",
                  "suggestedRevisionCasual": "string (Äoáº¡n vÄƒn viáº¿t láº¡i theo phong cÃ¡ch Giao tiáº¿p / Casual / Idiomatic)"
                }
            `;

        const result = await model.generateContent(prompt);
        const response = await result.response;
        const textResponse = response.text();
        const jsonData = JSON.parse(textResponse);

        this.modelHealth[modelName] = { status: 'ok', reason: null, lastTried: new Date() };

        return jsonData;

      } catch (error) {
        console.error(`âš ï¸ AI Service Error (evaluating writing, Model: ${selectedModelId}):`, error.message);
        // Handle error logic similarly to analyzeText
        if (error.message.includes('Quota')) {
          continue;
        }
        break;
      }
    }

    // Default error response
    return {
      score: 0,
      grammarFeedback: [],
      vocabularyUsage: "Há»‡ thá»‘ng AI Ä‘ang quÃ¡ táº£i hoáº·c gáº·p lá»—i. Vui lÃ²ng thá»­ láº¡i sau.",
      suggestedRevisionFormal: text,
      suggestedRevisionCasual: text
    };
  }

  async generateRoleplayResponse(messages, context, modelId = null) {
    const prompt = `You are playing a role in a conversation to help an English learner practice. 
Your role/context is: ${context || 'A friendly native English speaker having a casual chat'}.

Here is the conversation history:
${messages.map(m => `${m.role === 'user' ? 'Learner' : 'You'}: ${m.content}`).join('\n')}

Instructions for your response:
1. Act exclusively as your assigned role. Do NOT break character.
2. Respond naturally and conversationally, as a human would in real life.
3. Keep your response relatively short (1-3 sentences) to encourage back-and-forth dialogue.
4. Do NOT provide translation, grammar explanations, or feedback unless explicitly asked.
5. If the learner makes a small grammar mistake, ignore it and just keep the conversation flowing naturally.
6. Simply output your next response directly. Do not include prefixes like "You:".`;

    // This method assumes a startChatAndSendMessage method exists or needs to be implemented.
    // For now, let's simulate it using generateContent directly for simplicity,
    // but a proper chat session management would be better.
    // If `startChatAndSendMessage` is a planned method, it should be added.
    // For this change, I'll assume a direct call to generateContent for the prompt.

    const modelsToTry = modelId
      ? [modelId, ...this.fallbackModels.filter(m => m !== modelId)]
      : this.fallbackModels;

    for (const selectedModelId of modelsToTry) {
      try {
        let modelName = selectedModelId;
        if (modelName.startsWith('models/')) {
          modelName = modelName.replace('models/', '');
        }

        console.log(`ðŸ’¬ Attempting roleplay response with model: ${modelName}`);

        const model = genAI.getGenerativeModel({
          model: modelName,
          generationConfig: this.generationConfig
        });

        const result = await model.generateContent(prompt);
        const response = await result.response;
        const textResponse = response.text();

        this.modelHealth[modelName] = { status: 'ok', reason: null, lastTried: new Date() };
        return textResponse;

      } catch (error) {
        console.error(`âš ï¸ AI Service Error (generating roleplay response, Model: ${selectedModelId}):`, error.message);
        if (error.message.includes('Quota')) {
          continue;
        }
        break;
      }
    }
    return "I'm sorry, I'm having trouble responding right now. Please try again later.";
  }

  async evaluateDictation(originalText, userInput, accuracyScore, modelId = null) {
    const modelsToTry = modelId
      ? [modelId, ...this.fallbackModels.filter(m => m !== modelId)]
      : this.fallbackModels;

    for (const selectedModelId of modelsToTry) {
      try {
        let modelName = selectedModelId;
        if (modelName.startsWith('models/')) {
          modelName = modelName.replace('models/', '');
        }

        console.log(`ðŸŽ§ Attempting dictation evaluation with model: ${modelName}`);

        const model = genAI.getGenerativeModel({
          model: modelName,
          generationConfig: this.generationConfig
        });

        const prompt = `
          Báº¡n lÃ  giÃ¡o viÃªn tiáº¿ng Anh cháº¥m bÃ i nghe chÃ©p chÃ­nh táº£ (dictation).
          NgÆ°á»i há»c vá»«a nghe cÃ¢u gá»‘c: "${originalText}"
          Há» Ä‘Ã£ gÃµ láº¡i: "${userInput}"
          Tá»· lá»‡ Ä‘Ãºng lÃ  ${accuracyScore}%.
          
          Nhiá»‡m vá»¥ cá»§a báº¡n:
          HÃ£y nháº­n xÃ©t ngáº¯n gá»n (dÆ°á»›i 50 chá»¯ báº±ng tiáº¿ng Viá»‡t) vá» lá»—i sai chÃ­nh táº£ hoáº·c ngá»¯ phÃ¡p há» máº¯c pháº£i vÃ  cÃ¡ch kháº¯c phá»¥c.
          Náº¿u há» gÃµ Ä‘Ãºng tuyá»‡t Ä‘á»‘i (100%), hÃ£y dÃ nh má»™t lá»i khen ngáº¯n gá»n.
          
          YÃªu cáº§u output (JSON schema tÄ©nh, khÃ´ng tráº£ lá»i markdown hay kÃ½ tá»± thá»«a nÃ o ngoÃ i JSON):
          {
            "feedback": "string (nháº­n xÃ©t ngáº¯n gá»n)"
          }
        `;

        const result = await model.generateContent(prompt);
        const response = await result.response;
        const textResponse = response.text();
        const jsonData = JSON.parse(textResponse);

        this.modelHealth[modelName] = { status: 'ok', reason: null, lastTried: new Date() };

        return jsonData.feedback || jsonData.Feedback;

      } catch (error) {
        console.error(`âš ï¸ AI Service Error (evaluating dictation, Model: ${selectedModelId}):`, error.message);
        if (error.message.includes('Quota')) {
          continue;
        }
        break;
      }
    }

    return "Há»‡ thá»‘ng AI Ä‘ang gáº·p lá»—i hoáº·c quÃ¡ táº£i. HÃ£y thá»­ tá»± so sÃ¡nh cÃ¢u cá»§a báº¡n vá»›i Ä‘Ã¡p Ã¡n nhÃ©!";
  }
}

const aiService = new AIService();

const analyzeTextWithGemini = async (text, modelId) => {
  return await aiService.analyzeText(text, modelId);
};

const analyzeMediaWithGemini = async (filePath, modelId) => {
  return await aiService.analyzeMedia(filePath, modelId);
};

const evaluateWritingWithGemini = async (text, targetWords, modelId) => {
  return await aiService.evaluateWriting(text, targetWords, modelId);
};

const evaluateDictationWithGemini = async (originalText, userInput, accuracyScore, modelId) => {
  return await aiService.evaluateDictation(originalText, userInput, accuracyScore, modelId);
};

const getAvailableModels = async () => {
  return await aiService.listModels();
};

module.exports = { analyzeTextWithGemini, analyzeMediaWithGemini, evaluateWritingWithGemini, getAvailableModels, evaluateDictationWithGemini };